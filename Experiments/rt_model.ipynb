{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddh\\.conda\\envs\\testenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0076, Val Loss: 0.0189, Accuracy: 0.9952\n",
      "Epoch [2/10], Loss: 0.0017, Val Loss: 0.0121, Accuracy: 0.9985\n",
      "Epoch [3/10], Loss: 0.0010, Val Loss: 0.0151, Accuracy: 0.9953\n",
      "Epoch [4/10], Loss: 0.0011, Val Loss: 0.0026, Accuracy: 0.9995\n",
      "Epoch [5/10], Loss: 0.0007, Val Loss: 0.0013, Accuracy: 0.9998\n",
      "Epoch [6/10], Loss: 0.0007, Val Loss: 0.0005, Accuracy: 1.0000\n",
      "Epoch [7/10], Loss: 0.0060, Val Loss: 0.0075, Accuracy: 0.9981\n",
      "Epoch [8/10], Loss: 0.0003, Val Loss: 0.0021, Accuracy: 0.9993\n",
      "Epoch [9/10], Loss: 0.0003, Val Loss: 0.0006, Accuracy: 0.9998\n",
      "Epoch [10/10], Loss: 0.0003, Val Loss: 0.0088, Accuracy: 0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddh\\.conda\\envs\\testenv\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 99 into shape (1,50,99)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16660\\1366467793.py\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m# Assuming the length of landmarks is 99 (3 coordinates for each of the 33 landmarks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m99\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming we need a sequence of 50 frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 99 into shape (1,50,99)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import mediapipe as mp\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "angles_df = pd.read_csv('angles.csv')\n",
    "labels_df = pd.read_csv('labels.csv')\n",
    "landmarks_df = pd.read_csv('landmarks.csv')\n",
    "xyz_distances_df = pd.read_csv('xyz_distances.csv')\n",
    "calculated_distances_df = pd.read_csv('calculated_3d_distances.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "def create_sequences(angles_df, landmarks_df, labels_df, seq_length=50):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    for vid_id in angles_df['vid_id'].unique():\n",
    "        angles_seq = angles_df[angles_df['vid_id'] == vid_id].iloc[:, 1:].values  # Exclude vid_id\n",
    "        landmarks_seq = landmarks_df[landmarks_df['vid_id'] == vid_id].iloc[:, 1:].values  # Exclude vid_id\n",
    "        label = labels_df[labels_df['vid_id'] == vid_id]['class'].values[0]\n",
    "\n",
    "        # Generate sequences\n",
    "        for start in range(len(angles_seq) - seq_length):\n",
    "            end = start + seq_length\n",
    "            angles_subseq = angles_seq[start:end]\n",
    "            landmarks_subseq = landmarks_seq[start:end]\n",
    "\n",
    "            # Combine angles and landmarks\n",
    "            combined = np.hstack((angles_subseq, landmarks_subseq))\n",
    "            sequences.append(combined)\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Create sequences for training\n",
    "sequences, labels = create_sequences(angles_df, landmarks_df, labels_df)\n",
    "\n",
    "# Step 2: Preprocess the Labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)  # Convert to numeric labels\n",
    "one_hot_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Step 3: Split the Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(sequences, one_hot_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Define Dataset Class\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = PoseDataset(X_train, y_train)\n",
    "val_dataset = PoseDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 5: Define the Spatial-Temporal Transformer Model\n",
    "class SpatialTemporalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SpatialTemporalTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=4, num_encoder_layers=3)\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        x = x.permute(1, 0, 2)  # Change to (sequence_length, batch_size, input_dim)\n",
    "        tgt = x  # Use the same tensor as target\n",
    "        x = self.transformer(x, tgt)  # Forward pass through the transformer\n",
    "        x = x.mean(dim=0)  # Take the mean over the sequence\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Step 6: Train the Model\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += loss_fn(outputs, labels).item()\n",
    "                predicted = torch.argmax(outputs, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "            \n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {correct / total:.4f}')\n",
    "\n",
    "# Instantiate the model and train\n",
    "model = SpatialTemporalTransformer(input_dim=X_train.shape[2], num_classes=len(label_encoder.classes_))\n",
    "train_model(model, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "# Step 7: Real-Time Pose Correction Feedback\n",
    "mp_pose = mp.solutions.pose\n",
    "cap = cv2.VideoCapture(0)  # Use the default camera\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_rgb.flags.writeable = False\n",
    "\n",
    "    # Get pose landmarks\n",
    "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5) as pose:\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Draw landmarks on the frame\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Prepare input for the model\n",
    "        landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.pose_landmarks.landmark]).flatten()\n",
    "        # Assuming the length of landmarks is 99 (3 coordinates for each of the 33 landmarks)\n",
    "        input_data = np.concatenate([landmarks]).reshape(1, 50, 99)  # Assuming we need a sequence of 50 frames\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "        # Get model predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(input_tensor)\n",
    "            predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "            exercise_name = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "        # Display the predicted exercise\n",
    "        cv2.putText(frame, f'Predicted Exercise: {exercise_name}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Pose Correction Feedback', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 99 into shape (1,50,99)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16660\\3102850470.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Reshape to (1, 1, 99) for a single frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m99\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 1 sample, 1 frame, 99 landmarks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 99 into shape (1,50,99)"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "cap = cv2.VideoCapture(0)  # Use the default camera\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_rgb.flags.writeable = False\n",
    "\n",
    "    # Get pose landmarks\n",
    "    with mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5) as pose:\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Draw landmarks on the frame\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Capture landmarks for the current frame\n",
    "        landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.pose_landmarks.landmark]).flatten()\n",
    "\n",
    "        # Reshape to (1, 1, 99) for a single frame\n",
    "        input_data = landmarks.reshape(1, 50, 99)  # 1 sample, 1 frame, 99 landmarks\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "        # Get model predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(input_tensor)\n",
    "            predicted_class = torch.argmax(prediction, dim=1).item()\n",
    "            exercise_name = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "        # Display the predicted exercise\n",
    "        cv2.putText(frame, f'Predicted Exercise: {exercise_name}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Pose Correction Feedback', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
