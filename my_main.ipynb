{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 # number of input steps\n",
    "output_window = 1 # number of prediction steps, in this model its fixed to one\n",
    "block_len = input_window + output_window # for one input-output pair\n",
    "batch_size = 10\n",
    "train_size = 0.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inout_sequences(input_data, input_window ,output_window):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    block_num =  L - block_len + 1\n",
    "    # total of [N - block_len + 1] blocks\n",
    "    # where block_len = input_window + output_window\n",
    "\n",
    "    for i in range( block_num ):\n",
    "        train_seq = input_data[i : i + input_window]\n",
    "        train_label = input_data[i + output_window : i + input_window + output_window]\n",
    "        # print(\"train_label=\", train_label)\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "\n",
    "    return torch.FloatTensor(np.array(inout_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len angles = 11817\n",
      "9453\n",
      "9453\n",
      "torch.Size([9353, 2, 100, 7])\n"
     ]
    }
   ],
   "source": [
    "EXERCISE = \"squat\"\n",
    "SLICE = \"yes\"\n",
    "def get_data():\n",
    "    # get the data\n",
    "    angles = pd.read_csv(\"data/angles.csv\")\n",
    "    labels = pd.read_csv(\"data/labels.csv\")\n",
    "\n",
    "    # get data only for a single exercise\n",
    "    if SLICE == \"yes\":\n",
    "        labels = labels[labels[\"class\"]==EXERCISE]\n",
    "        angles = angles[angles[\"vid_id\"].isin(labels[\"vid_id\"])]\n",
    "\n",
    "    # transform data using MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    angles1 = angles.drop([\"vid_id\",\"frame_order\"], axis=1)\n",
    "    angles2 = scaler.fit_transform(angles1)\n",
    "    # angles2=angles\n",
    "    print(\"len angles =\",len(angles2))\n",
    "    \n",
    "    # make 80:20 split in train and test data\n",
    "    samples = int(len(angles2) * train_size) # use a parameter to control training size\n",
    "    print(samples)\n",
    "\n",
    "    train_data = angles2[:samples]\n",
    "    test_data = angles2[samples:]\n",
    "    print(len(train_data))\n",
    "    train_sequence = create_inout_sequences( train_data,input_window ,output_window)\n",
    "    test_data = create_inout_sequences(test_data,input_window,output_window)\n",
    "    print(train_sequence.shape)\n",
    "\n",
    "    return train_sequence.to(device),test_data.to(device), scaler\n",
    "\n",
    "train_data, val_data, scaler= get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 45.95745914,  81.40655851,  35.31698093, 118.72569764,\n",
       "        145.04757687, 108.92687528, 110.06666011],\n",
       "       [ 44.07924366,  79.32359956,  37.19674018, 103.62295977,\n",
       "        137.77669768, 103.7313326 , 104.62871574],\n",
       "       [ 34.98290598,  78.51498663,  41.72450673,  86.89696227,\n",
       "        123.90273624,  93.21553859, 105.45896413],\n",
       "       [ 30.29175084,  78.18402059,  39.90260294,  87.05849974,\n",
       "        124.05389663,  87.08307928, 101.91383038],\n",
       "       [ 21.01956678,  77.0753296 ,  40.87256018,  86.13374144,\n",
       "        120.80011468,  61.59622844, 102.00630054],\n",
       "       [ 37.24937281,  82.61837173,  37.53021555,  85.91353527,\n",
       "        122.3634294 ,  94.45488108, 104.65253366],\n",
       "       [ 33.53204606,  76.23924776,  41.23229977,  79.63009053,\n",
       "        112.47110252,  93.66927818,  92.19466333],\n",
       "       [ 35.76064817,  70.20299304,  39.07817938,  77.7497086 ,\n",
       "        105.33941569, 105.75657502,  92.23103024],\n",
       "       [ 34.72743573,  69.19033211,  34.96156806,  70.85173171,\n",
       "         86.47063353, 104.47075851,  91.24256586],\n",
       "       [ 41.99378113,  77.24890775,  33.0099011 ,  64.01752334,\n",
       "         76.9591175 , 110.00951515, 107.84009443],\n",
       "       [ 47.91984599,  77.13379764,  33.08744455,  64.0421132 ,\n",
       "         71.70387947, 108.94375274,  98.61781941],\n",
       "       [ 48.81424244,  73.67586328,  30.70118199,  66.27497236,\n",
       "         69.82400757, 106.34292627,  97.76381607],\n",
       "       [ 75.53007608,  97.28785289,  26.32655197,  55.92125345,\n",
       "         59.71817062, 118.08832456, 111.23514384],\n",
       "       [ 70.86176239,  92.50072465,  25.91737583,  56.67867429,\n",
       "         61.05501677, 114.38921129, 100.46428477],\n",
       "       [ 61.14785789,  84.85567529,  26.45665497,  55.05013197,\n",
       "         58.45584233, 111.25015183,  94.95421515],\n",
       "       [ 81.53981118, 103.8554115 ,  24.37472707,  48.24288047,\n",
       "         54.35841258, 118.4787551 , 110.10116985],\n",
       "       [ 51.07430119,  73.87144832,  24.95333432,  53.18449559,\n",
       "         55.47882851, 107.4377124 ,  84.65190794],\n",
       "       [ 72.5018846 ,  90.18394647,  23.76135293,  53.49238085,\n",
       "         55.06006076, 115.32588392, 103.78605476],\n",
       "       [ 71.7650779 ,  92.53014127,  23.53568648,  51.07161777,\n",
       "         50.41666715, 114.71113482, 105.00479744],\n",
       "       [ 54.30830828,  73.4151097 ,  24.00652549,  59.52734578,\n",
       "         60.86265089, 111.98355235,  92.35290031],\n",
       "       [ 67.89079049,  87.46597921,  25.8395835 ,  42.58906091,\n",
       "         48.32377273, 109.41403034,  92.80504195],\n",
       "       [ 58.46945062,  79.43583046,  25.65329063,  50.92283504,\n",
       "         51.48626775, 110.17613633,  94.94365972],\n",
       "       [ 63.67428383,  84.04822146,  25.95086719,  45.84358519,\n",
       "         50.11984027, 109.69864524,  96.77687309],\n",
       "       [ 59.20255215,  85.90600409,  26.67923911,  51.23586191,\n",
       "         59.12682118, 116.07656501, 110.79953834],\n",
       "       [ 65.72931064,  88.41661014,  26.33957041,  47.22280122,\n",
       "         53.81027014, 115.43238909, 109.08402954],\n",
       "       [ 67.93880845,  90.42994941,  26.89879599,  46.61551383,\n",
       "         55.18685915, 117.70937196, 110.36154056],\n",
       "       [ 74.92557866,  97.02210832,  26.1271282 ,  45.00554715,\n",
       "         50.67852401, 116.19077165, 109.08902395],\n",
       "       [ 57.39652887,  80.35486419,  25.41723337,  52.58814196,\n",
       "         61.72998325, 114.47039496, 101.97323876],\n",
       "       [ 32.40687606,  57.90592652,  25.67770259,  58.82852571,\n",
       "         69.70435818, 100.26112057,  92.42655747],\n",
       "       [ 44.77379541,  67.52023786,  24.66661524,  59.0881091 ,\n",
       "         68.51752283, 113.53863387, 103.93260313],\n",
       "       [ 55.79198163,  75.38000523,  23.215141  ,  56.1388418 ,\n",
       "         60.64302851, 113.94961227, 102.86752271],\n",
       "       [ 91.46850367, 115.33652947,  21.55002441,  44.26801086,\n",
       "         48.48325736, 117.68297905, 122.20021859],\n",
       "       [ 86.44038849, 111.04022155,  21.96814235,  47.4662413 ,\n",
       "         48.51719583, 120.01043518, 117.2533954 ],\n",
       "       [ 94.18336558, 114.06184472,  20.09644305,  47.97607584,\n",
       "         46.16038402, 119.0704104 , 120.56050741],\n",
       "       [ 80.77742014, 100.6626554 ,  22.60985312,  53.42059164,\n",
       "         54.89657824, 118.30262908, 111.92306184],\n",
       "       [ 90.47267082, 114.45050607,  22.73672065,  50.52173197,\n",
       "         52.48028559, 118.65684362, 122.55337424],\n",
       "       [ 73.66642805, 104.16777384,  25.1747291 ,  58.70868544,\n",
       "         64.81850185, 116.3161592 , 122.36249517],\n",
       "       [ 57.5582876 ,  88.27513199,  30.57949551,  67.97991775,\n",
       "         83.30743286, 116.03435544, 105.52007229],\n",
       "       [ 42.87769647,  67.78390241,  35.02526028,  78.16099038,\n",
       "         92.03921605, 108.2604436 ,  82.60333132],\n",
       "       [ 44.66283751,  73.99061261,  31.29226445,  86.90980584,\n",
       "        110.83545621, 109.02679283,  95.72297765],\n",
       "       [ 38.31074988,  73.77600045,  31.42035007,  86.76789949,\n",
       "        114.20640363, 104.56386566, 102.11792996],\n",
       "       [ 48.0823191 ,  84.69083532,  31.72664706,  83.05130957,\n",
       "        109.11795317, 111.1855486 , 112.24012338],\n",
       "       [ 43.61934922,  76.32594157,  32.68577315,  90.83203926,\n",
       "        124.62520015, 109.31729522, 104.01107633],\n",
       "       [ 46.765741  ,  77.56884918,  33.76766261, 100.30652716,\n",
       "        134.24000449, 111.98588613, 106.13399737],\n",
       "       [ 45.6927766 ,  70.70395939,  34.56151678, 102.8542368 ,\n",
       "        136.89726386, 111.66926664,  97.94118083],\n",
       "       [ 48.46178199,  70.19927759,  41.50807044, 135.79363835,\n",
       "        150.3894804 , 115.93472431,  88.323455  ],\n",
       "       [ 48.02931116,  67.53372885,  46.54776306, 156.87996686,\n",
       "        155.9202138 , 115.18920936,  87.41101908],\n",
       "       [ 46.61649456,  72.78916772,  40.03659472, 153.54521465,\n",
       "        151.07442878, 116.37260482,  97.27068034],\n",
       "       [ 48.55405943,  75.40345172,  46.84778057, 159.71746684,\n",
       "        154.88900375, 115.49675894,  92.52611005],\n",
       "       [ 52.01817101,  76.88021032,  51.16460778, 160.93360395,\n",
       "        155.65395105, 117.88996399,  88.17581017],\n",
       "       [ 47.52871315,  74.54620982,  48.1822229 , 157.99970923,\n",
       "        152.84467937, 115.15207046,  90.52259861],\n",
       "       [ 44.50755084,  74.16821847,  45.53608221, 155.32375195,\n",
       "        148.75945885, 113.99745473,  95.08119506],\n",
       "       [ 50.0187431 ,  76.95898101,  51.75465882, 161.39806273,\n",
       "        155.31649604, 117.27359185,  97.0729287 ],\n",
       "       [ 49.62614418,  74.1503875 ,  53.91246936, 159.71965098,\n",
       "        151.25535525, 114.64756057,  85.60007598],\n",
       "       [ 46.9577382 ,  77.14310743,  51.69424274, 156.24306557,\n",
       "        146.17734345, 113.10914436,  97.51246888],\n",
       "       [ 47.56442674,  74.55020049,  53.56608324, 159.74692659,\n",
       "        150.0675756 , 114.93871006,  86.56086122],\n",
       "       [ 51.27564395,  79.75559194,  56.43046461, 159.78741155,\n",
       "        147.58668541, 114.14853507,  94.04537814],\n",
       "       [ 50.13419518,  79.66915218,  55.72230477, 163.52178809,\n",
       "        155.13049907, 116.67914663,  96.71309984],\n",
       "       [ 50.46005837,  78.70428928,  57.26587009, 162.74419271,\n",
       "        154.40370368, 116.77923391,  94.14727055],\n",
       "       [ 52.89462803,  81.97913987,  58.54294993, 162.58426982,\n",
       "        150.87266605, 118.33707353,  96.25556507],\n",
       "       [ 52.69211774,  79.64312816,  59.71970646, 164.76052947,\n",
       "        155.47973735, 117.89174615,  90.35607782],\n",
       "       [ 51.71379782,  78.48899966,  57.17966602, 162.82708549,\n",
       "        151.38895365, 117.62710627,  89.97140803],\n",
       "       [ 50.13392329,  78.84466127,  55.35167181, 161.90072828,\n",
       "        151.58516346, 117.39757914,  90.67359054],\n",
       "       [ 49.7549722 ,  77.35105084,  52.7325348 , 160.80777036,\n",
       "        149.50371359, 117.009249  ,  88.55300932],\n",
       "       [ 45.90385943,  71.4443499 ,  50.78685297, 157.69520468,\n",
       "        149.72594307, 118.07175473,  79.149608  ],\n",
       "       [ 47.84313028,  73.12037193,  49.34937355, 154.74721233,\n",
       "        146.65205091, 116.26995039,  92.84154526],\n",
       "       [ 47.63626975,  74.49013936,  52.09804878, 155.52098286,\n",
       "        146.27154798, 114.1523752 ,  83.98194257],\n",
       "       [ 45.60020063,  70.95201058,  40.06968882, 149.6040346 ,\n",
       "        142.46885891, 118.31556033,  83.74682724],\n",
       "       [ 51.74780543,  73.93939023,  48.87443037, 157.97141992,\n",
       "        151.47636329, 120.30920127,  77.72812975],\n",
       "       [ 42.42207765,  17.67723242,  43.88631613, 138.4076246 ,\n",
       "        164.8300649 , 121.25915522,  44.19708265],\n",
       "       [ 48.6093118 ,  67.50364007,  67.32754641,  85.69038207,\n",
       "         73.99232679, 155.13369637, 178.58030398],\n",
       "       [106.77267519, 132.72927585,  80.4932917 ,  84.65882041,\n",
       "        108.14263151, 137.96982651, 134.38004888],\n",
       "       [173.80970077, 144.70317241,  76.2787693 ,  29.60403678,\n",
       "         22.22473952, 140.07735584, 152.23353693],\n",
       "       [ 65.49048433,  40.73859727,  57.19832259,  57.2874169 ,\n",
       "         68.84801874, 161.63000236, 120.04221073],\n",
       "       [124.20663268, 171.09783736,  91.8468372 ,  62.63968155,\n",
       "         50.58530484, 109.14219818, 153.1270969 ],\n",
       "       [ 88.5734105 , 135.33305392,  58.7319087 , 107.58744427,\n",
       "         68.09366406, 132.71893084, 166.55285587],\n",
       "       [134.73032243, 149.1801824 ,  64.57005023, 118.28365503,\n",
       "        133.49927037, 139.0377954 , 136.9728858 ],\n",
       "       [ 95.42863378, 107.01454862,  16.98997979, 136.08248816,\n",
       "        152.53957702, 126.81373913, 124.84888596],\n",
       "       [ 46.64597599,  87.98824853,  54.95402081, 121.89692157,\n",
       "        119.59810889,  80.73996994,  96.42652942],\n",
       "       [ 79.66946186,  85.93985067,  46.59190609, 143.38242504,\n",
       "        110.44127115,  78.78783844,  54.54291788],\n",
       "       [ 10.50479477,  29.68922557,  74.93747793,  96.95019482,\n",
       "         53.45466295, 114.16280294, 117.91475694],\n",
       "       [ 36.25235643, 106.76200395,  70.15026878,  48.55317978,\n",
       "         33.12077618,  15.98696207,  86.60176088],\n",
       "       [ 20.63717041,  95.07349831,  69.67652216,  56.5235795 ,\n",
       "         33.88870266,  55.87843385,  88.51046235],\n",
       "       [ 76.1254092 ,  89.33306635,  47.25705244, 131.73847549,\n",
       "        164.25836107, 110.24686043, 125.55337157],\n",
       "       [ 89.98850884,  88.50030301,  62.50563261, 130.16512188,\n",
       "        136.91508245,  70.41469713,  55.17196261],\n",
       "       [ 34.80920274,  89.67390321,  89.95385132,  60.43285962,\n",
       "         46.43256407,  75.4378001 ,  47.55293219],\n",
       "       [ 76.15324863,  63.44825413,  46.76426156, 109.34188288,\n",
       "        150.95818709, 131.05913692,  79.07878905],\n",
       "       [ 82.83813532,  66.61914095,  97.74536608,  54.4098343 ,\n",
       "         54.1330628 ,  86.78010709,  22.59228507],\n",
       "       [ 64.68131216,  63.64768164,  61.27301168,  49.06766476,\n",
       "         39.52852619, 102.44091218,  64.96474485],\n",
       "       [ 48.55179367,  49.55468253,  90.22811913,  42.58605641,\n",
       "         62.67086864,  95.74262826,  43.2684857 ],\n",
       "       [ 96.92239906,  90.10819789,  52.58152095,  51.05294286,\n",
       "         45.13139616, 126.28808734,  91.48431243],\n",
       "       [ 80.92542975,  79.99157057,  45.42751707,  80.42269795,\n",
       "         77.75019224, 125.58537601,  91.78896138],\n",
       "       [ 92.7173651 ,  79.35931444,  97.62029338,  39.31548289,\n",
       "         94.45028424, 114.11232968,  70.74748236],\n",
       "       [ 58.30942452,  39.3087052 ,  17.12144924, 152.7159001 ,\n",
       "        171.39123854,  52.22385862,  63.06602173],\n",
       "       [ 56.12412494,  55.41327847,  28.50032446,  50.95986091,\n",
       "         53.5457677 , 104.08269357, 109.81105298],\n",
       "       [ 52.31094887,  53.65221394,  28.33869005,  50.87160705,\n",
       "         52.67803503, 104.51622476, 109.40586515],\n",
       "       [ 55.98557818,  54.87911654,  28.27965479,  49.67391494,\n",
       "         50.86609139, 104.67940892, 107.70439528],\n",
       "       [ 51.55586155,  52.46375724,  28.61973271,  52.41926829,\n",
       "         55.14348278, 104.53381296, 108.29290758],\n",
       "       [ 62.91848745,  63.03099123,  28.11092964,  50.80468251,\n",
       "         49.37933709, 108.40008842, 112.75168389],\n",
       "       [ 50.79111945,  53.45241594,  27.90424522,  52.57555965,\n",
       "         52.33457548, 106.29389571, 110.88531431]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0][0][-1])\n",
    "scaler.inverse_transform(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(val_data[0][0])\n",
    "# (val_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(input_data, i, batch_size):\n",
    "    batch_len = min(batch_size, len(input_data) - i)\n",
    "    \n",
    "    # Extract the input and target tensors from the batch\n",
    "    data = input_data[i:i + batch_len]\n",
    "    \n",
    "    # Extracting the inputs and targets\n",
    "    inputs = torch.stack([item[0] for item in data]).permute(1, 0, 2)  # Shape: [100, batch_len, 9]\n",
    "    targets = torch.stack([item[1] for item in data]).permute(1, 0, 2)  # Shape: [100, batch_len, 9]\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1          [-1, 10, 10, 512]           4,096\n",
      "              ReLU-2          [-1, 10, 10, 512]               0\n",
      "            Linear-3          [-1, 10, 10, 512]         262,656\n",
      "              ReLU-4          [-1, 10, 10, 512]               0\n",
      "            Linear-5            [-1, 10, 10, 7]           3,591\n",
      "================================================================\n",
      "Total params: 270,343\n",
      "Trainable params: 270,343\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.57\n",
      "Params size (MB): 1.03\n",
      "Estimated Total Size (MB): 2.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class MyNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=7, input_size=7, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linera_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,7)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.linera_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "# model = LSTM().to(device)\n",
    "# summary(model, (10,7))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming the input has a sequence length of 10 and input size of 7\n",
    "sequence_length = 10\n",
    "input_size = 7\n",
    "model = MyNN().to(device)\n",
    "summary(model, (batch_size, sequence_length, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "for batch, i in enumerate(range(0, len(train_data), batch_size)):  # Now len-1 is not necessary\n",
    "    # data and target are the same shape with (input_window,batch_len,1)\n",
    "    data, targets = get_batch(train_data, i , batch_size)\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2907,  0.3137,  0.2512,  0.2176,  0.2039,  0.3596,  0.3431],\n",
       "        [ 0.4960,  0.4223, -0.0091,  0.3137,  0.0999,  0.5591,  0.4925],\n",
       "        [ 0.7441,  0.7393,  0.2833,  0.2641,  0.2324,  0.7450,  0.7131],\n",
       "        [ 0.2590,  0.1392,  0.2351,  0.3388,  0.2816,  0.2953,  0.2948],\n",
       "        [ 0.2597,  0.1787,  0.2314,  0.3713,  0.3636,  0.3318,  0.3435],\n",
       "        [ 0.1076,  0.1642,  0.2959,  0.5887,  0.5596,  0.3688,  0.3069],\n",
       "        [ 0.5681,  0.4148,  0.2414,  0.3416,  0.2998,  0.4355,  0.4008],\n",
       "        [ 0.2004,  0.3365,  0.1990,  0.7395,  0.7002,  0.6139,  0.5765],\n",
       "        [ 0.0184,  0.1498,  0.0972,  0.4392,  0.4903,  0.4561,  0.4614],\n",
       "        [ 0.2927,  0.1936,  0.2136,  0.3943,  0.4047,  0.3308,  0.2385]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(100,10,7)\n",
    "pred = model(dummy_input)\n",
    "# (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "pred.argmax(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    # Calculate the total sum of squares\n",
    "    ss_total = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    \n",
    "    # Calculate the residual sum of squares\n",
    "    ss_residual = torch.sum((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    \n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch\n",
    "data, targets = get_batch(train_data, 10, batch_size)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_data, optimizer, loss_fn, epoch):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    for batch, i in enumerate(range(0, len(train_data), batch_size)):  # Now len-1 is not necessary\n",
    "        # data and target are the same shape with (input_window,batch_len,1)\n",
    "        data, targets = get_batch(train_data, i , batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(data_source) - 1, eval_batch_size): # Now len-1 is not necessary\n",
    "        for i in range(0, len(data_source), eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            total_loss += len(data[0]) * loss_fn(output, targets).cpu().item()\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    data, _ = get_batch(data_source , 0 , 1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps):            \n",
    "            output = eval_model(data[-input_window:])\n",
    "            # (seq-len , batch-size , features-num)\n",
    "            # input : [ m,m+1,...,m+n ] -> [m+1,...,m+n+1]\n",
    "            data = torch.cat((data, output[-1:])) # [m,m+1,..., m+n+1]\n",
    "\n",
    "    data = data.cpu().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10 # The number of epochs\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   187/  935 batches | lr 0.001000 | 11.31 ms | loss 0.01403 | ppl     1.01\n",
      "| epoch   1 |   374/  935 batches | lr 0.001000 | 12.52 ms | loss 0.01351 | ppl     1.01\n",
      "| epoch   1 |   561/  935 batches | lr 0.001000 | 11.52 ms | loss 0.01647 | ppl     1.02\n",
      "| epoch   1 |   748/  935 batches | lr 0.001000 | 12.99 ms | loss 0.00659 | ppl     1.01\n",
      "| epoch   1 |   935/  935 batches | lr 0.001000 | 11.98 ms | loss 0.00938 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.17s | valid loss 0.01422 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   187/  935 batches | lr 0.000902 | 11.97 ms | loss 0.01432 | ppl     1.01\n",
      "| epoch   2 |   374/  935 batches | lr 0.000902 | 12.18 ms | loss 0.01342 | ppl     1.01\n",
      "| epoch   2 |   561/  935 batches | lr 0.000902 | 11.87 ms | loss 0.01641 | ppl     1.02\n",
      "| epoch   2 |   748/  935 batches | lr 0.000902 | 12.13 ms | loss 0.00652 | ppl     1.01\n",
      "| epoch   2 |   935/  935 batches | lr 0.000902 | 12.37 ms | loss 0.00941 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.20s | valid loss 0.01434 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   187/  935 batches | lr 0.000857 | 12.91 ms | loss 0.01425 | ppl     1.01\n",
      "| epoch   3 |   374/  935 batches | lr 0.000857 | 12.29 ms | loss 0.01360 | ppl     1.01\n",
      "| epoch   3 |   561/  935 batches | lr 0.000857 | 11.91 ms | loss 0.01620 | ppl     1.02\n",
      "| epoch   3 |   748/  935 batches | lr 0.000857 | 12.18 ms | loss 0.00650 | ppl     1.01\n",
      "| epoch   3 |   935/  935 batches | lr 0.000857 | 11.97 ms | loss 0.00942 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.33s | valid loss 0.01440 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   187/  935 batches | lr 0.000815 | 12.61 ms | loss 0.01425 | ppl     1.01\n",
      "| epoch   4 |   374/  935 batches | lr 0.000815 | 12.58 ms | loss 0.01359 | ppl     1.01\n",
      "| epoch   4 |   561/  935 batches | lr 0.000815 | 11.81 ms | loss 0.01616 | ppl     1.02\n",
      "| epoch   4 |   748/  935 batches | lr 0.000815 | 11.77 ms | loss 0.00657 | ppl     1.01\n",
      "| epoch   4 |   935/  935 batches | lr 0.000815 | 12.67 ms | loss 0.00959 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.35s | valid loss 0.01569 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   187/  935 batches | lr 0.000774 | 11.79 ms | loss 0.01430 | ppl     1.01\n",
      "| epoch   5 |   374/  935 batches | lr 0.000774 | 12.15 ms | loss 0.01352 | ppl     1.01\n",
      "| epoch   5 |   561/  935 batches | lr 0.000774 | 11.74 ms | loss 0.01621 | ppl     1.02\n",
      "| epoch   5 |   748/  935 batches | lr 0.000774 | 11.68 ms | loss 0.00651 | ppl     1.01\n",
      "| epoch   5 |   935/  935 batches | lr 0.000774 | 11.68 ms | loss 0.00940 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 11.17s | valid loss 0.01569 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   187/  935 batches | lr 0.000735 | 12.42 ms | loss 0.01412 | ppl     1.01\n",
      "| epoch   6 |   374/  935 batches | lr 0.000735 | 11.99 ms | loss 0.01352 | ppl     1.01\n",
      "| epoch   6 |   561/  935 batches | lr 0.000735 | 12.05 ms | loss 0.01619 | ppl     1.02\n",
      "| epoch   6 |   748/  935 batches | lr 0.000735 | 12.83 ms | loss 0.00651 | ppl     1.01\n",
      "| epoch   6 |   935/  935 batches | lr 0.000735 | 12.05 ms | loss 0.00944 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.34s | valid loss 0.01379 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   187/  935 batches | lr 0.000698 | 12.10 ms | loss 0.01392 | ppl     1.01\n",
      "| epoch   7 |   374/  935 batches | lr 0.000698 | 12.18 ms | loss 0.01330 | ppl     1.01\n",
      "| epoch   7 |   561/  935 batches | lr 0.000698 | 14.89 ms | loss 0.01596 | ppl     1.02\n",
      "| epoch   7 |   748/  935 batches | lr 0.000698 | 15.86 ms | loss 0.00653 | ppl     1.01\n",
      "| epoch   7 |   935/  935 batches | lr 0.000698 | 12.01 ms | loss 0.00929 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 13.42s | valid loss 0.01390 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   187/  935 batches | lr 0.000663 | 11.88 ms | loss 0.01379 | ppl     1.01\n",
      "| epoch   8 |   374/  935 batches | lr 0.000663 | 12.07 ms | loss 0.01322 | ppl     1.01\n",
      "| epoch   8 |   561/  935 batches | lr 0.000663 | 11.79 ms | loss 0.01578 | ppl     1.02\n",
      "| epoch   8 |   748/  935 batches | lr 0.000663 | 13.26 ms | loss 0.00640 | ppl     1.01\n",
      "| epoch   8 |   935/  935 batches | lr 0.000663 | 13.83 ms | loss 0.00921 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.64s | valid loss 0.01351 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   187/  935 batches | lr 0.000630 | 15.61 ms | loss 0.01358 | ppl     1.01\n",
      "| epoch   9 |   374/  935 batches | lr 0.000630 | 17.72 ms | loss 0.01307 | ppl     1.01\n",
      "| epoch   9 |   561/  935 batches | lr 0.000630 | 13.53 ms | loss 0.01558 | ppl     1.02\n",
      "| epoch   9 |   748/  935 batches | lr 0.000630 | 12.09 ms | loss 0.00645 | ppl     1.01\n",
      "| epoch   9 |   935/  935 batches | lr 0.000630 | 12.22 ms | loss 0.00914 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 14.18s | valid loss 0.01298 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   187/  935 batches | lr 0.000599 | 13.40 ms | loss 0.01354 | ppl     1.01\n",
      "| epoch  10 |   374/  935 batches | lr 0.000599 | 12.45 ms | loss 0.01299 | ppl     1.01\n",
      "| epoch  10 |   561/  935 batches | lr 0.000599 | 12.27 ms | loss 0.01541 | ppl     1.02\n",
      "| epoch  10 |   748/  935 batches | lr 0.000599 | 11.89 ms | loss 0.00642 | ppl     1.01\n",
      "| epoch  10 |   935/  935 batches | lr 0.000599 | 11.85 ms | loss 0.00911 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 11.67s | valid loss 0.01298 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data=train_data, optimizer=optimizer, loss_fn=loss_fn, epoch=epoch, model=model)\n",
    "    if ( epoch % 5 == 0 ):\n",
    "        predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "   \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() \n",
    "    # using adam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41.99378113,  77.24890775,  33.0099011 ,  64.01752334,\n",
       "         76.9591175 , 110.00951515, 107.84009443],\n",
       "       [ 47.91984599,  77.13379764,  33.08744455,  64.0421132 ,\n",
       "         71.70387947, 108.94375274,  98.61781941],\n",
       "       [ 48.81424244,  73.67586328,  30.70118199,  66.27497236,\n",
       "         69.82400757, 106.34292627,  97.76381607],\n",
       "       [ 75.53007608,  97.28785289,  26.32655197,  55.92125345,\n",
       "         59.71817062, 118.08832456, 111.23514384],\n",
       "       [ 70.86176239,  92.50072465,  25.91737583,  56.67867429,\n",
       "         61.05501677, 114.38921129, 100.46428477],\n",
       "       [ 61.14785789,  84.85567529,  26.45665497,  55.05013197,\n",
       "         58.45584233, 111.25015183,  94.95421515],\n",
       "       [ 81.53981118, 103.8554115 ,  24.37472707,  48.24288047,\n",
       "         54.35841258, 118.4787551 , 110.10116985],\n",
       "       [ 51.07430119,  73.87144832,  24.95333432,  53.18449559,\n",
       "         55.47882851, 107.4377124 ,  84.65190794],\n",
       "       [ 72.5018846 ,  90.18394647,  23.76135293,  53.49238085,\n",
       "         55.06006076, 115.32588392, 103.78605476],\n",
       "       [ 71.7650779 ,  92.53014127,  23.53568648,  51.07161777,\n",
       "         50.41666715, 114.71113482, 105.00479744]])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, targets = get_batch(train_data, 10 , batch_size)\n",
    "scaler.inverse_transform(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1059, 0.1531, 0.2079, 0.2792, 0.2168, 0.0627, 0.7748]])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n",
      "torch.Size([100, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    data, _ = get_batch(data_source , 0 , 1)\n",
    "    print(data[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps):            \n",
    "            output = eval_model(data[-input_window:])\n",
    "            print(output.shape)\n",
    "            # (seq-len , batch-size , features-num)\n",
    "            # input : [ m,m+1,...,m+n ] -> [m+1,...,m+n+1]\n",
    "            data = torch.cat((data, output[-1:])) # [m,m+1,..., m+n+1]\n",
    "\n",
    "    data = data.cpu().view(-1)\n",
    "\n",
    "predict_future(model, val_data,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to be used for later use\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from torchvision.io import read_image\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, EXERCISE, annotations_file=\"data/labels.csv\", data_file=\"data/angles.csv\", SLICE=\"no\", transform=\"yes\"):\n",
    "#         self.labels = pd.read_csv(annotations_file)\n",
    "#         self.angles = pd.read_csv(data_file)\n",
    "#         # self.data_file = data_file\n",
    "#         if SLICE == \"yes\":\n",
    "#             self.labels = self.labels[self.labels[\"class\"]==EXERCISE]\n",
    "#             self.angles = self.angles[self.angles[\"vid_id\"].isin(self.labels[\"vid_id\"])]\n",
    "#         if transform == \"yes\":\n",
    "#             scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#             self.angles1 = self.angles.drop([\"vid_id\",\"frame_order\"], axis=1)\n",
    "#             self.tangles = scaler.fit_transform(self.angles1)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.angles)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.data_file, self.labels.iloc[idx, 0])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
